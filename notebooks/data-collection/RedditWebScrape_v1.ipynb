{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e707fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\thatb\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: idna in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\thatb\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d8b056a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import date,timedelta,datetime\n",
    "from dateutil.parser import parse\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4882c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Discission Threads\n",
    "# Turns off notifications for our driver so our scrape doesn't break at any point\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "chrome_options.add_experimental_option(\"prefs\",prefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "382bb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def select_sort_filter(filtr:str = \"New\"):\n",
    "    \"\"\"\n",
    "    Args: filtr\n",
    "    Select how you want the results to be filtered.\n",
    "    Options are New, Hot, Top, Relevance, and Most Comments\n",
    "    \"New\" by default\n",
    "    \"\"\"\n",
    "    filter_select_button = driver.find_element_by_xpath('//button[@data-testid = \"search-results-filter-sort\"]')\n",
    "    filter_select_button.click()\n",
    "    \n",
    "    filter_button = driver.find_element_by_xpath(f'//button[@role = \"menuitem\"]//span[text()=\"{filtr}\"]')\n",
    "    filter_button.click()\n",
    "def pull_Comments(filtr_comments: str = \"top\"):\n",
    "    \"\"\"\n",
    "    Args: filtr_comments\n",
    "    Select how you want post comments to be filtered.\n",
    "    Options are best, top, new, controversial, old, or q&a\n",
    "    Returns:\n",
    "    List of comments on the selected post\n",
    "    This includes the comment_id, comment_author, text, and number of upvotes\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    # Tries to click the comment drop down menu\n",
    "    try:\n",
    "        comments_sort = driver.find_element(By.XPATH,('//span[@class=\"_2-cXnP74241WI7fpcpfPmg _3LRBCA71BwLLXBNsSlY7HW\"]'))\n",
    "        driver.execute_script(\"arguments[0].click();\", comments_sort)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print(\"sort dropdown error\")\n",
    "    #tries to click the desired sort value from arg:filtr_comments\n",
    "    try:\n",
    "        comments_sort_select = driver.find_element(By.XPATH,(f'//button[@class=\"_10K5i7NW6qcm-UoCtpB3aK _3LwUIE7yX7CZQKmD2L87vf _1oYEKCssGFjqxQ9jJMNj5G\"]//span[text() =\"{filtr_comments}\"]'))\n",
    "        driver.execute_script(\"arguments[0].click();\", comments_sort_select)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"sort select error\")\n",
    "        \n",
    "    #pull the value displayed for sorting on the website and checks if it matches our desired sort\n",
    "    sorted_ = driver.find_element(By.XPATH,('//span[@class=\"_2-cXnP74241WI7fpcpfPmg _3LRBCA71BwLLXBNsSlY7HW\"]'))\n",
    "    sorted_by = \"\".join(sorted_.text.split(' ')[-1:])\n",
    "    \n",
    "    if sorted_by.lower() != filtr_comments:\n",
    "            print('comments not properly sorted')\n",
    "    \n",
    "    # we are done with buttons so we create a soup and pull all comment objects\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    comment_cells = soup.find('div', class_ = \"_1YCqQVO-9r-Up6QPB9H6_4 _1YCqQVO-9r-Up6QPB9H6_4\").find_all('div', class_ = \"_3sf33-9rVAO_v4y0pIW_CH\")\n",
    "    \n",
    "    # Loop through all comment objects\n",
    "    for ix, cell in enumerate(comment_cells):\n",
    "        # Checks for a username to ensure this is a proper comment\n",
    "        if cell.find('div', class_= \"_2mHuuvyV9doV3zwbZPtIPG\"):\n",
    "            \n",
    "            comment = {}\n",
    "            comment_id = cell.get('id')\n",
    "            \n",
    "            # Checks that this is not a \"x more replies\" option\n",
    "            if 'moreComments' not in comment_id:\n",
    "                \n",
    "                comment.update({'comment_id':comment_id})\n",
    "                #takes the username from the comment\n",
    "                username = cell.find('div', class_= \"_2mHuuvyV9doV3zwbZPtIPG\").find('a')\n",
    "                comment.update({'username':username.get('href')})\n",
    "                \n",
    "                #comments are classed line by line, this finds them all and pastes them together\n",
    "                comment_body =cell.find_all('p')\n",
    "                line_body = \"\"\n",
    "                for line in comment_body:\n",
    "                    line_body = line_body + \" \" + line.text\n",
    "                \n",
    "                comment.update({'body':line_body})\n",
    "                \n",
    "                #gets the number of upvotes for that comment, if there are any at all\n",
    "                comment_upvotes = cell.find('div', class_ = \"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\")\n",
    "                if comment_upvotes:\n",
    "                    comment.update({\"upvotes\":comment_upvotes.text})\n",
    "\n",
    "                comments.append(comment)\n",
    "            \n",
    "    return comments    \n",
    "    \n",
    "def pull_Daily_Discussion(filtr_comments: str = \"top\"):\n",
    "    \"\"\"\n",
    "    Args: filtr_comments\n",
    "    Select how you want post comments to be filtered.\n",
    "    Options are best, top, new, controversial, old, or q&a\n",
    "    Returns: \n",
    "    List of posts from the specified url \"Daily Discussion\"\n",
    "    \"\"\"\n",
    "    # This worked when I last tried it, but I didn't add any of the bug fixes that I did to pull_Posts\n",
    "    # Plus I think pull_Posts works on daily discussion as well\n",
    "    posts = []\n",
    "    \n",
    "    links = driver.find_elements(By.XPATH,('//h3'))\n",
    "    \n",
    "    for ix,link in enumerate(links):\n",
    "        post = {}\n",
    "        post.update({'title':link.text}) \n",
    "        date = \"\".join(link.text.split(' ')[-3:])\n",
    "        post.update({\"Reference_Date\":parse(date)})\n",
    "        \n",
    "        link.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        post_body = driver.find_element(By.XPATH,('//p[@class=\"_1qeIAgB0cPwnLhDF9XSiJM\"]'))\n",
    "        post.update({'body':post_body.text})\n",
    "        time.sleep(1)\n",
    "        \n",
    "        num_comments = driver.find_element(By.XPATH,('//span[@class=\"FHCV02u6Cp2zYL0fhQPsO\"]'))\n",
    "        post.update({'num_comments': num_comments.text})\n",
    "        time.sleep(2)\n",
    "        nums_upvotes_rev = driver.find_elements(By.XPATH,('//span[@class=\"WBY6J5DJsZFZXSxBqtq0F\"]//span'))\n",
    "        if nums_upvotes_rev:\n",
    "            nums_upvotes = [num.text for num in reversed(nums_upvotes_rev)]\n",
    "            \n",
    "            num_upvotes = ''.join(nums_upvotes)\n",
    "            post.update({'num_upvotes':num_upvotes})\n",
    "        else:\n",
    "            num_upvotes = driver.find_element(By.XPATH,('//div[@class=\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ\"]'))\n",
    "            post.update({'num_upvotes':num_upvotes.text})\n",
    "        #time.sleep(5)\n",
    "        \n",
    "        post_author = driver.find_element(By.XPATH,('//a[@class=\"_2tbHP6ZydRpjI44J3syuqC  _23wugcdiaj44hdfugIAlnX oQctV4n0yUb0uiHDdGnmE\"]'))\n",
    "        post.update({'author':post_author.text})\n",
    "        \n",
    "        comments = pull_Comments(filtr_comments)\n",
    "        \n",
    "        post.update({'comments':comments})\n",
    "        \n",
    "        close_button = driver.find_element(By.XPATH,('//button[@title = \"Close\"]'))\n",
    "        close_button.click()\n",
    "        close_button.click()\n",
    "        \n",
    "        posts.append(post)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "    return(posts)\n",
    "\n",
    "def pull_Posts(filtr_comments: str = \"top\"):\n",
    "    \"\"\"\n",
    "    Args: filtr_comments\n",
    "    Select how you want post comments to be filtered.\n",
    "    Options are best, top, new, controversial, old, or q&a\n",
    "    Returns: \n",
    "    List of posts from the specified url\n",
    "    \"\"\"\n",
    "    \n",
    "    posts = []\n",
    "    \n",
    "    # First part here scrolls to the bottom of the page to load more posts\n",
    "    # This repeats until there are 100 posts, or until it has scrolled 100 times\n",
    "    # Finishes by returning to the top\n",
    "    i=0\n",
    "    page_length = []\n",
    "    while len(page_length) <= 100:\n",
    "        i += 1\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "        page_length = driver.find_elements(By.XPATH,('//div[@class = \"_14-YvdFiW5iVvfe5wdgmET\"]'))\n",
    "        if i == 100:\n",
    "            break\n",
    "    driver.execute_script(\"window.scrollTo(0,0)\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # pulls all the posts that have just been loaded\n",
    "    links = driver.find_elements(By.XPATH,\n",
    "                                 ('//div[@class = \"_1poyrkZ7g36PawDueRza-J _11R7M_VOgKO1RJyRSRErT3 \"]'))\n",
    "    # for each post\n",
    "    for ix,link in enumerate(links):\n",
    "        post = {}\n",
    "        # this field only exists on promoted posts, we don't want to click those\n",
    "        is_promoted = link.find_elements(By.XPATH, \n",
    "                                         (\".//span[@class = '_2oEYZXchPfHwcf9mTMGMg8 V0WjfoF5BV7_qbExmbmeR']\"))\n",
    "        if not is_promoted:\n",
    "            action = ActionChains(driver)\n",
    "            # If the post title or link doesn't exist the loop is stopped\n",
    "            try:\n",
    "                post_link = link.find_element(By.XPATH,('.//h3'))\n",
    "            except: \n",
    "                print(f'no link at elem {ix}')\n",
    "                break\n",
    "            try:\n",
    "                post.update({'title':post_link.text})\n",
    "            except:\n",
    "                print(f'no text at elem {ix}')\n",
    "                break\n",
    "            # clicks the link to open the post\n",
    "            action.move_to_element(post_link).click().perform()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Pulls the data from the post into soup\n",
    "            soup_2 = BeautifulSoup(driver.page_source)\n",
    "            # if the structure isn't what we expect for the post, we clicked something wrong and the loop\n",
    "            # is broken\n",
    "            try:\n",
    "                date_elem = soup_2.find(\n",
    "                    \"div\", class_ = \"u35lf2ynn4jHsVUwPmNU Dx3UxiK86VcfkFQVHNXNi\").find(\n",
    "                \"a\", class_ = \"_3jOxDPIQ0KaOWpzvSQo-1s\") \n",
    "            except:\n",
    "                print(f'finished at elem {ix}')\n",
    "                break\n",
    "            # to get the datetime of the post, we parse the \"_ hours ago\" field of the post and\n",
    "            # subtract it from the current time\n",
    "            date_num = int(date_elem.text.split(' ')[0])\n",
    "            time_p = date_elem.text.split(' ')[1]\n",
    "            if time_p == 'hours' or time_p == 'hour':\n",
    "                time_d = timedelta(hours = date_num)\n",
    "                date_dt = datetime.now() - time_d\n",
    "            elif time_p == 'minutes' or time_p == 'minute':\n",
    "                time_d = timedelta(minutes = date_num)\n",
    "                date_dt = datetime.now() - time_d\n",
    "            elif time_p == 'seconds' or time_p == 'seconds':\n",
    "                time_d = timedelta(seconds = date_num)\n",
    "                date_dt = datetime.now() - time_d\n",
    "            elif time_p == 'day' or time_p == 'days':\n",
    "                time_d = timedelta(seconds = date_num)\n",
    "                date_dt = datetime.now() - time_d\n",
    "            else:\n",
    "                print (f\"unkown time delta {time_p}\")\n",
    "            post.update({\"Reference_Date\":date_dt})\n",
    "            \n",
    "            # pulls the body of the post\n",
    "            post_body = soup_2.find('h1', class_ = \"_eYtD2XCVieq6emjKBH3m\")\n",
    "            post.update({'body':post_body.text})\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # takes the number of comments on the post\n",
    "            num_comments = soup_2.find('span', class_ = \"FHCV02u6Cp2zYL0fhQPsO\")\n",
    "            post.update({'num_comments': num_comments.text})\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # takes the tag of the post\n",
    "            post_tags = soup_2.find(\n",
    "                \"div\", class_ = \"u35lf2ynn4jHsVUwPmNU Dx3UxiK86VcfkFQVHNXNi\").find(\n",
    "            \"div\", class_ = \"_2fiIRtMpITeCAzXc4cANKp _1mK-LVHGTTlcFpMsjItjYJ\").find(\n",
    "            'span')\n",
    "            post.update({'tag':post_tags.text})\n",
    "            \n",
    "            #takes the number of upvotes on the post\n",
    "            num_upvotes = soup_2.find(\"div\", class_ =\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ\")\n",
    "            post.update({'num_upvotes':num_upvotes.text})\n",
    "            \n",
    "            #takes the author of the post\n",
    "            post_author = driver.find_element(By.XPATH,('//a[@class=\"_2tbHP6ZydRpjI44J3syuqC  _23wugcdiaj44hdfugIAlnX oQctV4n0yUb0uiHDdGnmE\"]'))\n",
    "            post.update({'author':post_author.text})\n",
    "            \n",
    "            #run the function for pulling comments\n",
    "            comments = pull_Comments(filtr_comments)\n",
    "\n",
    "            post.update({'comments':comments})\n",
    "            #closes the post (we need to click twice for this)\n",
    "            close_button = driver.find_element(By.XPATH,('//button[@title = \"Close\"]'))\n",
    "            close_button.click()\n",
    "            close_button.click()\n",
    "            #update our list\n",
    "            posts.append(post)\n",
    "\n",
    "            time.sleep(5)\n",
    "            \n",
    "    posts_dict = {'main': posts}        \n",
    "    return(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "global driver\n",
    "url = \"https://www.reddit.com/r/wallstreetbets/search?q=flair_name%3A%22Daily%20Discussion%22&restrict_sr=1&sort=new\"\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.get(url)\n",
    "select_sort_filter(\"Hot\")\n",
    "time.sleep(10)\n",
    "posts_list = pull_Daily_Discussion()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc12be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.reddit.com/r/wallstreetbets/\"\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "posts_dict = pull_Posts()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "84e148de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('posts_hot_ctop_2_16_22.pkl', 'wb') as file:\n",
    "    pickle.dump(posts_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c40a050c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'What Are Your Moves Tomorrow, February 17, 2022',\n",
       " 'Reference_Date': datetime.datetime(2022, 2, 16, 16, 17, 21, 799255),\n",
       " 'body': 'What Are Your Moves Tomorrow, February 17, 2022',\n",
       " 'num_comments': '12.1k comments',\n",
       " 'tag': 'Daily Discussion',\n",
       " 'num_upvotes': '292',\n",
       " 'author': 'u/AutoModerator',\n",
       " 'comments': [{'comment_id': 't1_hx84k5q',\n",
       "   'username': '/user/SomeDumbassSays/',\n",
       "   'body': ' Silly me for investing in calls on a company that manufactures very very high end GPUs and other essential computer components.  I shouldâ€™ve bought calls in checks notes the food delivery company that delivers the wrong food, cold, to the wrong address',\n",
       "   'upvotes': '125'},\n",
       "  {'comment_id': 't1_hx84oxj',\n",
       "   'username': '/user/NrdRage/',\n",
       "   'body': \" Don't forget the previously bankrupted electric car that's going to a rental model and missed top and bottom and has no path to profitability.\",\n",
       "   'upvotes': '24'},\n",
       "  {'comment_id': 't1_hx8fxvn',\n",
       "   'username': '/user/Jackol4ntrn/',\n",
       "   'body': ' nvdia crushed earnings: flat dash failed miserably: moon fuck this clown market.',\n",
       "   'upvotes': '87'},\n",
       "  {'comment_id': 't1_hx8g1ph',\n",
       "   'username': '/user/HereComesThe_Squeeze/',\n",
       "   'body': ' Nvda will spike in the am',\n",
       "   'upvotes': '20'},\n",
       "  {'comment_id': 't1_hx8g7mm',\n",
       "   'username': '/user/CyborgAlgoInvestor/',\n",
       "   'body': ' Spike down ',\n",
       "   'upvotes': '13'},\n",
       "  {'comment_id': 't1_hx8975u',\n",
       "   'username': '/user/Burlaka/',\n",
       "   'body': ' NVDA pc nerds in shambles crying into their lgbt keyboards. DASH chads ordering steak dinner while getting a blowjob tonight.',\n",
       "   'upvotes': '82'},\n",
       "  {'comment_id': 't1_hx89dvy',\n",
       "   'username': '/user/avl0/',\n",
       "   'body': ' you can order prostitutes on dash now?',\n",
       "   'upvotes': '23'},\n",
       "  {'comment_id': 't1_hx8zmjs',\n",
       "   'username': '/user/bearhunter429/',\n",
       "   'body': ' That one day when Trump pumped markets saying that he had a very constructive and good phone call with Xi and markets rose 2%, later on China denied such phone call ever took place and we pumped another 1% instead of going down. The bull market was glorious while it lasted.',\n",
       "   'upvotes': '77'},\n",
       "  {'comment_id': 't1_hx8zvhz',\n",
       "   'username': '/user/lordbell21/',\n",
       "   'body': ' Kinda like Putin saying he withdrew some troops and we pumped 2%, and then we found out he added more and pumped more?',\n",
       "   'upvotes': '27'},\n",
       "  {'comment_id': 't1_hx833u0',\n",
       "   'username': '/user/TuaTurnsdaballova/',\n",
       "   'body': ' Please join my NVDA $300 prayer circle Æª(Ë˜âŒ£Ë˜)Êƒ',\n",
       "   'upvotes': '68'},\n",
       "  {'comment_id': 't1_hx80s8t',\n",
       "   'username': '/user/k767/',\n",
       "   'body': ' NVDA wants to burst through the ceiling so bad',\n",
       "   'upvotes': '65'},\n",
       "  {'comment_id': 't1_hx820n9',\n",
       "   'username': '/user/ZiGgyOwl/',\n",
       "   'body': ' Whoever said dash puts, thanks cause I inversed you',\n",
       "   'upvotes': '58'},\n",
       "  {'comment_id': 't1_hx8333s',\n",
       "   'username': '/user/collegekid234432/',\n",
       "   'body': ' You guys are literally just gambling with NVDA options, no strategy to it',\n",
       "   'upvotes': '57'},\n",
       "  {'comment_id': 't1_hx834ba',\n",
       "   'username': '/user/Zephym/',\n",
       "   'body': ' Welcome to the sub',\n",
       "   'upvotes': '52'},\n",
       "  {'comment_id': 't1_hx834dc',\n",
       "   'username': '/user/jahjue/',\n",
       "   'body': ' Why are u hear',\n",
       "   'upvotes': '22'},\n",
       "  {'comment_id': 't1_hx8dlqy',\n",
       "   'username': '/user/clutchhutch82/',\n",
       "   'body': ' I lost so much money today ðŸ¤™ðŸ¼',\n",
       "   'upvotes': '51'},\n",
       "  {'comment_id': 't1_hx8dxp6',\n",
       "   'username': '/user/FullTard2000/',\n",
       "   'body': ' i come from the future to give you this warning 10 years from now every GME shareholder who is holding shares as of Feb 16th 2022 will be homeless on the street yelling \"paper handed bitch\" at every person who walks by',\n",
       "   'upvotes': '50'}]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('posts_hot_ctop_2_16_22.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)\n",
    "test['main'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
